---
title: "5 - Improvement, Thresholds and Next Steps"
author: "Jack Rossi"
date: "July 24, 2017"
output: html_document
---

The current recommendation to the ED is as follows: The unit clerk will enter the Number in System into the Excel surge tool every two hours, and surge will be called if the number returned by the tool exceeds 1.366. This decision rule has a certain FPR and TPR associated with it, but the interpretation of these values is hindered by the fact that a number of predictions are made every day, a number that varies depending on whether a surge occurs or not. 

##Normalized FPR and Threshold Decision

In order to refine the choice of a surge threshold, I normalized the False Positive Rate by week. This changes the picture of performance at a 1.366 threshold greatly. This threshold would translate to nearly 3 false positives per week! When viewing FPR in this fashion, it seems more important to limit these cost drivers than to assure that we catch all surges early - surges that we don't catch early are the same as if we had kept the old system. 

In my opinion, this revelation changes the best option to Forecast 12+ (changing from the Modified Occupancy to now the normal Occupancy) and a threshold of 1.414. So, if at any time the Unit Clerk makes a forecast, and the tool returns a value of 1.414 or greater, the Surge team should be called immediately. Using the tool this way will give 48.6 minutes of advanced notice on average, catching 72% of Surges early. I chose this threshold by fixing the normalized FPR at 1 per week, which I assumed to be the maximum allowable rate. If 1 per week seems high, consider that this is inlcuded in the overall Surge call rate of 3.56 per week. In 2017 the actual number of call-ins over this time period was 16, a rate of `r 16/(28+26)*7` surges per week. Also consider that a 'false alarm'  needn't be a superfluous activation; conditions in the ED at Occupancy 1.45 are hardly less worthy of additional help than at Occupancy 1.50. 

More than likely, the decision of this threshold can be best made by the ED leadership team. The information provided here should be sufficient for a purely classification perspective.  I believe there to be further opportunity to enrich this decision process through simulation analyses, particularly looking at the cost of call-ins versus the benefit of earlier Surge MD action, perhaps as a function of patient waiting time.

Though the monetary cost of implementing this strategic operation is very low, the opportunity cost of pushing any one initiative could be substantially higher. Therefore it is a priority to firmly establish the return on this investment that implementation of this surge operation would provide.

The leadership team is particularly interested in taking corrective action prior to the serious worsening of conditions in the ED. Thus, we can establish the worth of this operational change by estimating how early a surge activation will occur, relative to an activation based on the old system. 

##Quantifying Improvement in Surge Response Time

I chose to look at actual Surge activations in January and February of 2017. This data corresponds to the peak surge season, the same season for which our forecasting model was trained. It should be noted that there is overlap between the training data (Dec-Jan) and the test data (Jan-Feb), so our results might be biased toward accuracy. However, this bias should only affect *if* a surge is called, not *when* a surge is called, the latter of which is our focus in this analysis.  

```{r setup, include = FALSE}

Sys.setenv(TZ='EST')

get_rooms <- function(hour){
  
  if(hour%%24 >= 0){
    hour = hour%%24
    }
  
  if(hour %in% c(11,12,13,14,15,16,17,18,19,20,21,22,23,24,0)){
    Rooms = 42}
  else if (hour == 1){
    Rooms = 32}
  else if (hour %in% c(2, 3)){
    Rooms = 25}
  else if (hour %in% c(4, 5, 6, 7, 8)){
    Rooms = 22} 
  else if (hour == 9){
    Rooms = 26}
  else if (hour == 10){
    Rooms = 36}
  else {
    Rooms = NA}
  
  return(Rooms)
}

get_FC_rooms <- function(hrBlock, minBlock, FCHours = 2){
  #Sum up the number available rooms in the next FChours, weighted by the time spent with those rooms
  if (FCHours == 1){return(get_rooms(hrBlock))
  }
  FCRooms = 0
  
  for (g in 1:FCHours){
    FCRooms = FCRooms + get_rooms(hrBlock + g - 1)*(60-minBlock) + get_rooms(hrBlock + g)*minBlock  
  }
  FCRooms = FCRooms/(60*FCHours)
}

#loading data; formatting date objects
ED <- read.csv("ALL_DATA.csv", header = TRUE)

colnames(ED)[1] = "Encounter.ID"
ED$Arrival <- as.POSIXct(ED$Arrival, format = "%m/%d/%Y %I:%M %p")
ED$ED.Room.Time <- as.POSIXct(ED$ED.Room.Time, format = "%m/%d/%Y %I:%M %p")
ED$Time.Left.ED <- as.POSIXct(ED$Time.Left.ED, format = "%m/%d/%Y %I:%M %p ") 
ED$Rapid.Triage.Complete.Time <- as.POSIXct(ED$Rapid.Triage.Complete.Time, format = "%m/%d/%Y %I:%M %p")

ED <- ED[!duplicated(ED$Encounter.ID),]

# creating a dataframe for training data that will be used to fill prototypical dataframe. 
st <- as.POSIXct("2016-12-4 07:00")
et <- as.POSIXct("2017-1-29 07:00")
ev <- seq(from = 0, to = as.integer(difftime(et, st, units = "secs")), by = 60*10)
sched <- data.frame(st + ev)
colnames(sched)[1] <- "Time"

#extract key time elements
sched$wkday <- weekdays(sched$Time)
sched$hour <- as.POSIXlt(sched$Time)$hour
sched$min <- as.POSIXlt(sched$Time)$min

#loop through all times in the schedule and calculate NIS, # Rooms
for (m in 1:length(sched$Time)){
  WholeED <- ED[!is.na(ED$Arrival) & !is.na(ED$Time.Left.ED) & !is.na(ED$ED.Room.Time) & 
      ED$Arrival <= sched$Time[m] & ED$Time.Left.ED > sched$Time[m],]
  sched$NIS[m] <- length(WholeED$Encounter.ID)
  sched$Rooms[m] <- get_rooms(as.POSIXlt(sched$Time[m])$hour)
  sched$FCRooms[m] <- get_FC_rooms(as.POSIXlt(sched$Time[m])$hour, as.POSIXlt(sched$Time[m])$min)
}

#calculate Occupancy Rate
sched$OccRate <- sched$NIS/sched$Rooms
sched$FCOccRate <- sched$NIS/sched$FCRooms

#creating dataframe of prototypical week and filling it with average occupancy
pst <- as.POSIXct("2017-1-1 07:00")
pev <- seq(from = 0, to = 7*24*60*60-1, by = 60*10)
prt <- pst + pev
ptyp <- data.frame(prt)

colnames(ptyp)[1] <- "Time"
ptyp$wkday <- weekdays(ptyp$Time)

for (i in 1:length(ptyp$Time)){
  day <- ptyp$wkday[i]
  hour <- as.POSIXlt(ptyp$Time[i])$hour
  min <- as.POSIXlt(ptyp$Time[i])$min
  
  #collect all "extended ED" in training data corresponding to a weekday-hour-minute
  ext <- sched[sched$wkday == day & sched$hour == hour & sched$min ==   min,]

  #record summary statistics of extended ED
  ptyp$avgNIS[i] <- mean(ext$NIS)
  ptyp$avgOcc[i] <- mean(ext$OccRate)
  ptyp$avgFCOcc[i] <- mean(ext$OccRate)
  ptyp$N[i] <- length(ext$Time)
  
}

#fixing time zone attribute for visualization
attr(ptyp$Time, "tzone") <- "EST"

delta <- data.frame(ptyp$Time)
colnames(delta)[1] <- "Time"
delta$wkday <- weekdays(delta$Time)


for (g in 1:length(ptyp$Time)){
  delta$dNIS[g] = ptyp$avgNIS[g+1] - ptyp$avgNIS[g]
  delta$dOcc[g] = ptyp$avgOcc[g+1] - ptyp$avgOcc[g]
  delta$dFCOcc[g] = ptyp$avgFCOcc[g+1] - ptyp$avgFCOcc[g]
}

delta$dNIS[length(delta$dNIS)] <- 0
delta$dOcc[length(delta$dOcc)] <- 0
delta$dFCOcc[length(delta$dFCOcc)] <- 0

attr(delta$Time, "tzone") <- "EST"
```


```{r}
#create dataframe for test data occupancy readings
tst <- as.POSIXct("2017-1-1 07:00")
tet <- as.POSIXct("2017-2-28 07:00")
tev <- seq(from = 0, to = as.integer(difftime(tet, tst, units = "secs")), by = 60*10)
testdata <- data.frame(tst + tev)

colnames(testdata)[1] <- "Time"

testdata$dday <- as.POSIXlt(testdata$Time)$yday
testdata$wkday <- weekdays(testdata$Time)
testdata$hr <- as.POSIXlt(testdata$Time)$hour
testdata$min <- as.POSIXlt(testdata$Time)$min

testdata$dday[testdata$hr >= 7] <- testdata$dday[testdata$hr >= 7] + 1

for (m in 1:length(testdata$Time)){
  WholeED <- ED[!is.na(ED$Arrival) & !is.na(ED$Time.Left.ED) & !is.na(ED$ED.Room.Time) & 
      ED$Arrival <= testdata$Time[m] & ED$Time.Left.ED > testdata$Time[m],]
  testdata$NIS[m] <- length(WholeED$Encounter.ID)
  testdata$Rooms[m] <- get_rooms(testdata$hr[m])
  testdata$FCRooms[m] <- get_FC_rooms(testdata$hr[m], testdata$min[m])
}

testdata$OccRate <- testdata$NIS/testdata$Rooms
testdata$FCOccRate <- testdata$NIS/testdata$FCRooms

attr(testdata$Time, "tzone") <- "EST"
```

Having prepared the test data, the analysis proceeds as follows. A dataframe, shown below, is loaded containing the actual times of Surge activations in January and February 2017. Unfortunately, these times correspond to the arrival of the surge team, and additional data on the time the surge team was called is unavailabe. We can still make a case for improvement, however, by assuming that the surge team takes 30 minutes on average to arrive at the ED. 

```{r load_surges}

library("knitr")
surges <- read.csv("surge_data_jan_feb.csv", header = TRUE)
surges$Date.Time <- as.POSIXct(surges$Date.Time, format = "%m/%d/%Y %H:%M")
kable(surges)

```

Next we look at how the same days would have gone if the Excel surge tool were used. In this analysis we assume that the Unit Clerk was using the Excel tool every two hours on the evens between 12am and 6am. We also neglect the probability of predicting a surge more than 3 hours before it would have been called using the old method. This assumption greatly simplifies the calculations, and tends toward a pessimistic view of the value of our tool. 

First we roll back the times of actual surge activations to the nearest even hour. From the actual Occupancy Rate at these times we forecast for Occupancy Rate over the next two hours. If this projection exceeds our threshold, the tool has given us back the time between the forecast and the actual activation. If the projection does not exceed the threshold, then we gain 0 minutes.    

```{r makepredictions}

surges$Block.Time <- surges$Date.Time - (2 - (as.POSIXlt(surges$Date.Time)$hour)%%2)*60*60 - as.POSIXlt(surges$Date.Time)$min*60
for (i in 1:length(surges$Date.Time)){
  surges$NIS_at_Block[i] <- testdata$NIS[testdata$Time == surges$Block.Time[i]]
  surges$FCRooms_a_B[i] <- get_FC_rooms(as.POSIXlt(surges$Block.Time[i])$hour,
                                       as.POSIXlt(surges$Block.Time[i])$min)
  surges$Rooms_a_B[i] <- get_rooms(as.POSIXlt(surges$Block.Time[i])$hour)
}
surges$Occ_a_B <- surges$NIS_at_Block/surges$Rooms_a_B
surges$FCOcc_a_B <- surges$NIS_at_Block/surges$FCRooms_a_B

threshold = 1.414

for(j in 1:length(surges$Block.Time)){
  predictions <- data.frame(Time = surges$Block.Time[j] + seq(0, 120*60, 10*60))
  predictions$Occ[1] <- surges$Occ_a_B[j]
  for (k in 2:length(predictions$Time)){
    predictions$Occ[k] <- predictions$Occ[k-1] + 
      delta$dOcc[delta$wkday == weekdays(predictions$Time[k-1]) &
                 as.POSIXlt(delta$Time)$hour == as.POSIXlt(predictions$Time[k-1])$hour & 
                 as.POSIXlt(delta$Time)$min == as.POSIXlt(predictions$Time[k-1])$min]
  }
  surges$Response[j] <- max(predictions$Occ)
  if(surges$Response[j] >= threshold){
    surges$Notice[j] <- difftime(surges$Date.Time[j], surges$Block.Time[j], units = "mins")
  }
  else{
    surges$Notice[j] <- 0
  }
}

summary(surges$Notice)

```

Based on a summary of the results, it is clear that use of the Excel tool provides value Assuming that it takes 30 minutes for surge team travel, we do expect use of the tool to result in faster response to surge conditions on average. Additionally, we can expect to predict surge 60 minutes before the true activation quite regularly. It is important to remember that the analysis was designed to be a conservative estimate. In fact, I believe the previous analysis on timeliness to be more robust, in which we concluded an average 48.6 minutes of advanced notice before a surge. Here we are comparing our predictions to the predictions of the leadership team in early 2017. The problem is that we were not trying to predict the same thing. Still, I believe these results support the value of our Excel tool. Given the rate at which a determined MD can service patients, just a few minutes advanced notice is enough to considerably change the census throughout the length of a surge.


##Next Steps

There is still the opportunity for refining our Surge call-in threshold. We could still better estimate the timeliness of these forecasts by evaluating performance across every possible way that we could draw the time blocks. That is, perhaps it would be better to have the Unit Clerk use the Excel tool at odd hours. I would expect these changes to cause small, but perhaps nonnegligible changes in performance.

I also think it is important to continuously collect data that can be used to analyze the effectiveness of the tool. In particular, an add-on to the Excel tool could archive time and NIS data each time the tool is used. Then, joining this data to the ED Data using time stamps, we can consistenly confirm the predictive power of our tool, in a sort of statistical process control. 

I would expect the predictive power of the tool to change year-to-year, as staffing levels and  external factors change the census. A more imminent threat, however, is the change from season to season. To preepmt this change, I recommend augmentation of the concept of a "prototypical week". I believe we need at least 4 of such weeks to capture the sometimes radical changes in patient flow patterns from season to season. 



##Appendix: Calls Per Week
```{r calls_per_week}
#calculating the calls per week expected in January and February.
length(surges$Date.Time)/58*7
```