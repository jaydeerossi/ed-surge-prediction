---
title: "ED Population Projection"
author: "Jack Rossi"
date: "May 23, 2017"
output: html_document
---
This document outlines the code I used to create forecasts for ED population (number of patients in beds + wait room) based on an estimation of the average weekly pattern. This code is an adaptation of what was previously used to forecast Occupancy Rate. In future updates, We can simply divide the forecasts by number of rooms to allow backwards comparison to Occupancy Rate forecasts. 

```{r setup, include=FALSE}
library("ggplot2")
library("grid")

get_rooms <- function(hour){
  
  if(hour%%24 >= 0){
    hour = hour%%24
    }
  
  if(hour %in% c(11,12,13,14,15,16,17,18,19,20,21,22,23,24,0)){
    Rooms = 42}
  else if (hour == 1){
    Rooms = 32}
  else if (hour %in% c(2, 3)){
    Rooms = 25}
  else if (hour %in% c(4, 5, 6, 7, 8)){
    Rooms = 22} 
  else if (hour == 9){
    Rooms = 26}
  else if (hour == 10){
    Rooms = 36}
  else {
    Rooms = NA}
  
  return(Rooms)
}

get_FC_rooms <- function(hrBlock, minBlock, FCHours = 2){
  #Sum up the number available rooms in the next FChours, weighted by the time spent with those rooms
  if (FCHours == 1){return(get_rooms(hrBlock))
  }
  FCRooms = 0
  
  for (g in 1:FCHours){
    FCRooms = FCRooms + get_rooms(hrBlock + g - 1)*(60-minBlock) + get_rooms(hrBlock + g)*minBlock  
  }
  FCRooms = FCRooms/(60*FCHours)
}

# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
```

The modelling uses data from December 2016, and January, February, March 2017. 

```{r readdat, echo = FALSE}
#loading data; formatting date objects
ED <- read.csv("ALL_DATA.csv", header = TRUE)

colnames(ED)[1] = "Encounter.ID"
ED$Arrival <- as.POSIXct(ED$Arrival, format = "%m/%d/%Y %I:%M %p")
ED$ED.Room.Time <- as.POSIXct(ED$ED.Room.Time, format = "%m/%d/%Y %I:%M %p")
ED$Time.Left.ED <- as.POSIXct(ED$Time.Left.ED, format = "%m/%d/%Y %I:%M %p ") 
ED$Rapid.Triage.Complete.Time <- as.POSIXct(ED$Rapid.Triage.Complete.Time, format = "%m/%d/%Y %I:%M %p")

#often there are multiple rows for a single patient
ED <- ED[!duplicated(ED$Encounter.ID),]

```

## Calculating Prototypical Week in NIS

First I create a dataframe containing time stamps corresponding to QlikView (one of two primary ED data sources) data that will be used to train the model. I calculate the number of patients in the ED (those who have arrived, but not yet left) for each of the defined time stamps. I also calculate how many rooms are available at that time. Then I create a dataframe that will contain estimates of the weekly population shape. We are assuming that there is a static weekly cycle of population change; this implies that the mean NIS at any given time is a surjective function of three data: weekday, hour, and minute. 

The second for loop below captures the calculation of the average shape. First I extracted the needed datetime pieces (weekday, hour, minute) from the ith time stamp in the prototypical week dataframe. Then I found every time stamp in the training day with that same weekday, hour and minute.The NIS readings found at these related time stamps are then averaged. 

```{r fig.width=9, fig.height=4}
# creating a dataframe for training data that will be used to fill prototypical dataframe. 
st <- as.POSIXct("2016-12-4 07:00")
et <- as.POSIXct("2017-1-29 07:00")
ev <- seq(from = 0, to = as.integer(difftime(et, st, units = "secs")), by = 60*10)
sched <- data.frame(st + ev)
colnames(sched)[1] <- "Time"

#extract key time elements
sched$wkday <- weekdays(sched$Time)
sched$hour <- as.POSIXlt(sched$Time)$hour
sched$min <- as.POSIXlt(sched$Time)$min

#loop through all times in the schedule and calculate NIS, # Rooms
for (m in 1:length(sched$Time)){
  WholeED <- ED[!is.na(ED$Arrival) & !is.na(ED$Time.Left.ED) & !is.na(ED$ED.Room.Time) & 
      ED$Arrival <= sched$Time[m] & ED$Time.Left.ED > sched$Time[m],]
  sched$NIS[m] <- length(WholeED$Encounter.ID)
  sched$Rooms[m] <- get_rooms(as.POSIXlt(sched$Time[m])$hour)
}

#calculate Occupancy Rate
sched$OccRate <- sched$NIS/sched$Rooms

#creating dataframe of prototypical week and filling it with average occupancy
pst <- as.POSIXct("2017-1-1 07:00")
pev <- seq(from = 0, to = 7*24*60*60-1, by = 60*10)
prt <- pst + pev

ptyp <- data.frame(prt)
colnames(ptyp)[1] <- "Time"
ptyp$wkday <- weekdays(ptyp$Time)

for (i in 1:length(ptyp$Time)){
  day <- ptyp$wkday[i]
  hour <- as.POSIXlt(ptyp$Time[i])$hour
  min <- as.POSIXlt(ptyp$Time[i])$min
  
  #collect all "extended ED" in training data corresponding to a weekday-hour-minute
  ext <- sched[sched$wkday == day & sched$hour == hour & sched$min ==   min,]

  #record summary statistics of extended ED
  ptyp$avgNIS[i] <- mean(ext$NIS)
  ptyp$minNIS[i] <- min(ext$NIS)
  ptyp$maxNIS[i] <- max(ext$NIS)
  ptyp$N[i] <- length(ext$Time)
  
}

#fixing time zone attribute for visualization
attr(ptyp$Time, "tzone") <- "EST"

#show prototypical population shape
ggplot(ptyp , aes(Time, avgNIS)) + ylab("ED Population") + xlab("Weekday, Time") + geom_area(fill = "blue") + scale_x_datetime(date_labels = "%a, %R") + geom_vline(xintercept =  as.integer(as.POSIXct("2017-1-2 07:00"))) + geom_vline(xintercept = as.integer(as.POSIXct("2017-1-3 07:00"))) + geom_vline(xintercept = as.integer(as.POSIXct("2017-1-4 07:00"))) + geom_vline(xintercept = as.integer(as.POSIXct("2017-1-5 07:00"))) + geom_vline(xintercept = as.integer(as.POSIXct("2017-1-6 07:00"))) + geom_vline(xintercept = as.integer(as.POSIXct("2017-1-7 07:00")))
```
The resulting prototypical week is visualized above: The upper boundary of the area plot is a time series of the number in system, taken at 10-minute intervals. An alternate interpretation is that the differential slice of area at each time in the graph represents the amount of work left to be done in processing patients in the ED. Tick marks on the x axis show midnight, the beginning of the traditionally defined day.

Take note that the graph begins at 7am on the prototypical Sunday; subsequent 7am's are shown with black vertical lines. Since adjacent 7ams's roughly separate the data into the inverted U-shape that we associate with the daily cycle of ED filling, I will later define a day as the time between subsequent 7am's: thus, the first day of the week will be the 24 hours starting 07:00 Sunday, and so on.  

Next I created a new dataframe to hold NIS readings for the test data, which spans February and March. As before, I calculated NIS and # of Rooms at each of the ten minute intervals.

```{r, include=FALSE, warning=FALSE}
tst <- as.POSIXct("2017-1-29 07:00")
tet <- as.POSIXct("2017-3-26 07:00")
tev <- seq(from = 0, to = as.integer(difftime(tet, tst, units = "secs")), by = 60*10)
testdata <- data.frame(tst + tev)

colnames(testdata)[1] <- "Time"

testdata$dday <- as.POSIXlt(testdata$Time)$yday
testdata$wkday <- weekdays(testdata$Time)
testdata$hr <- as.POSIXlt(testdata$Time)$hour
testdata$min <- as.POSIXlt(testdata$Time)$min

testdata$dday[testdata$hr >= 7] <- testdata$dday[testdata$hr >= 7] + 1

for (m in 1:length(testdata$Time)){
  WholeED <- ED[!is.na(ED$Arrival) & !is.na(ED$Time.Left.ED) & !is.na(ED$ED.Room.Time) & 
      ED$Arrival <= testdata$Time[m] & ED$Time.Left.ED > testdata$Time[m],]
  testdata$NIS[m] <- length(WholeED$Encounter.ID)
  testdata$Rooms[m] <- get_rooms(testdata$hr)
}

testdata$OccRate <- testdata$NIS/testdata$Rooms

attr(testdata$Time, "tzone") <- "EST"
```



```{r, echo= FALSE}

week1 <- testdata[testdata$Time >= as.POSIXct("2017-1-29 07:00") & testdata$Time < as.POSIXct("2017-2-5 07:00"),]
week2 <- testdata[testdata$Time >= as.POSIXct("2017-2-5 07:00") & testdata$Time < as.POSIXct("2017-2-12 07:00"),]
week3 <- testdata[testdata$Time >= as.POSIXct("2017-2-12 07:00") & testdata$Time < as.POSIXct("2017-2-19 07:00"),]
week4 <- testdata[testdata$Time >= as.POSIXct("2017-2-19 07:00") & testdata$Time < as.POSIXct("2017-2-26 07:00"),]
week5 <- testdata[testdata$Time > as.POSIXct("2017-2-26 07:00") & testdata$Time < as.POSIXct("2017-3-5 07:00"),]
week6 <- testdata[testdata$Time >= as.POSIXct("2017-3-5 07:00") & testdata$Time < as.POSIXct("2017-3-12 07:00"),]
week7 <- testdata[testdata$Time >= as.POSIXct("2017-3-12 07:00") & testdata$Time < as.POSIXct("2017-3-19 07:00"),]
week8 <- testdata[testdata$Time >= as.POSIXct("2017-3-19 07:00") & testdata$Time < as.POSIXct("2017-3-26 07:00"),]

#create a time datum for each OccRate reading that can be used for comparison to the prototypical week.
#For example, the first data point in week 5 will be linked back to the first reading in the ptyp df. 

week1$CompTime <- as.POSIXct(as.numeric(week1$Time) - as.numeric(as.POSIXct("2017-1-29")) + as.numeric(as.POSIXct("2017-1-1")), origin = "1970-1-1")
week2$CompTime <- as.POSIXct(as.numeric(week2$Time) - as.numeric(as.POSIXct("2017-2-5")) + as.numeric(as.POSIXct("2017-1-1")), origin = "1970-1-1")
week3$CompTime <- as.POSIXct(as.numeric(week3$Time) - as.numeric(as.POSIXct("2017-2-12")) + as.numeric(as.POSIXct("2017-1-1")), origin = "1970-1-1")
week4$CompTime <- as.POSIXct(as.numeric(week4$Time) - as.numeric(as.POSIXct("2017-2-19")) + as.numeric(as.POSIXct("2017-1-1")), origin = "1970-1-1")
week5$CompTime <- as.POSIXct(as.numeric(week5$Time) - as.numeric(as.POSIXct("2017-2-26")) + as.numeric(as.POSIXct("2017-1-1")), origin = "1970-1-1")
week6$CompTime <- as.POSIXct(as.numeric(week6$Time) - as.numeric(as.POSIXct("2017-3-5")) + as.numeric(as.POSIXct("2017-1-1")), origin = "1970-1-1")
week7$CompTime <- as.POSIXct(as.numeric(week7$Time) - as.numeric(as.POSIXct("2017-3-12")) + as.numeric(as.POSIXct("2017-1-1")), origin = "1970-1-1")
week8$CompTime <- as.POSIXct(as.numeric(week8$Time) - as.numeric(as.POSIXct("2017-3-19")) + as.numeric(as.POSIXct("2017-1-1")), origin = "1970-1-1")


p1 <- ggplot(ptyp, aes(Time, avgNIS)) + ylab("1/29--2/5") + xlab(NULL) + geom_step(color = "gray") +  scale_x_datetime(date_labels = "%A") + geom_step(data = week1, aes(CompTime, NIS), color = "black") +theme_bw() + ylim(0, 70)
p2 <- ggplot(ptyp, aes(Time, avgNIS)) + ylab("2/5--2/12") + xlab(NULL) + geom_step(color = "gray")  + scale_x_datetime(date_labels = "%A") + geom_step(data = week2, aes(CompTime, NIS), color = "black") +theme_bw() + ylim(0, 70)
p3 <- ggplot(ptyp, aes(Time, avgNIS)) + ylab("2/12-2/19") + xlab(NULL) + geom_step(color = "gray") + scale_x_datetime(date_labels = "%A") + geom_step(data = week3, aes(CompTime, NIS), color = "black") +theme_bw() + ylim(0, 70)
p4 <- ggplot(ptyp, aes(Time, avgNIS)) + ylab("2/19--2/26") + xlab(NULL) + geom_step(color = "gray")  + scale_x_datetime(date_labels = "%A") + geom_step(data = week4, aes(CompTime, NIS), color = "black") +theme_bw() + ylim(0, 70)

p5 <- ggplot(ptyp, aes(Time, avgNIS)) + ylab("2/26--3/5") + xlab(NULL) + geom_step(color = "gray")  + scale_x_datetime(date_labels = "%A") + geom_step(data = week5, aes(CompTime, NIS), color = "black") +theme_bw() + ylim(0, 70)
p6 <- ggplot(ptyp, aes(Time, avgNIS)) + ylab("3/5--3/12") + xlab(NULL) + geom_step(color = "gray")  + scale_x_datetime(date_labels = "%A") + geom_step(data = week6, aes(CompTime, NIS), color = "black") +theme_bw() + ylim(0, 70)
p7 <- ggplot(ptyp, aes(Time, avgNIS)) + ylab("3/12--3/19") + xlab(NULL) + geom_step(color = "gray")  + scale_x_datetime(date_labels = "%A") + geom_step(data = week7, aes(CompTime, NIS), color = "black") +theme_bw() + ylim(0, 70)
p8 <- ggplot(ptyp, aes(Time, avgNIS)) + ylab("3/19--3/26") + xlab(NULL) + geom_step(color = "gray")  + scale_x_datetime(date_labels = "%A")+ geom_step(data = week8, aes(CompTime, NIS), color = "black") +theme_bw() + ylim(0, 70)

```
##Prototypical Week in NIS versus Test Data

In order to see how well the average NIS shape describes real data, I plotted each of the weeks of test data over the average shape. This first plot shows the month of February's NIS time series as black lines. The Y-axis labels denote the weeks of data shown, and the x axis ticks show midnight for the traditional days of week. Also reproduced on each of the 4 subplots is a gray line showing the NIS time series for the prototypical week.

```{r fig.width=8, fig.height=4, warning=FALSE, echo = FALSE, fig.align = "center"}
multiplot(p1, p2, p3, p4, layout = matrix(c(1,2,3,4), nrow = 4, byrow =TRUE)) 
```
Inherently, the prototypical NIS curve will be smoother than real data. However, in many cases the jaggedness of the test data does not obscure its similarity to the proposed average. There are several instances of irregularly pointed NIS profiles, something we might characterize as a surge. However, I believe it fair to claim as rule that a day's NIS progression will roughly follow an inverted U-shape. 
```{r fig.width=8, fig.height=4, echo = FALSE, fig.align = "center"}
multiplot(p5, p6, p7, p8, layout = matrix(c(1,2,3,4), nrow = 4, byrow =TRUE)) 
```
The same visualization for the available data in March tells much the same story. I would also note that the maximum NIS each day is close in time to the maximum suggested by the prototypical week, independent of the ultimate shape of NIS progression in that day.

##Forecasting NIS at Predefined Decision Points

Having established the suitability of the calculated prototypical week, I want to transform the data into a series of changes in NIS. The delta dataframe contains the change in NIS experienced over each prototypical 10 minute interval, E.G., $\Delta$NIS at 13:00 will be the difference between  NIS at 13:10 and 13:00. With these changes in hand, we can begin to apply them to readings of NIS in the test data, in order to forecast future values of NIS. 

```{r}
delta <- data.frame(ptyp$Time)
colnames(delta)[1] <- "Time"
delta$wkday <- weekdays(delta$Time)


for (g in 1:length(ptyp$Time)){
  delta$dNIS[g] = ptyp$avgNIS[g+1] - ptyp$avgNIS[g]
}

delta$dNIS[length(delta$dNIS)] <- 0
```

We start to make predictions assuming predefined decision points for Surge team activitation, say 4- and 8pm. The code goes through the test data looking for entries corresponding to 8 or 4pm. In these spots it fills in the predicted NIS as an average over the last two hours.The length of this initial forecast period will dictate how responsive the model is. 

Having filled in the initial reading for a day, the loop then creates an NIS prediction at every subsequent time within that day. Recall that our "days" are demarcated by 7am, So 4pm forecasts go from 4pm to the following 7am, and likewise, 8pm forecasts go from 8pm to the following 7am. 

```{r}
#NULL columns to be filled in with predictions
testdata$PredNIS_4p <- NA
testdata$PredNIS_8p <- NA
#predictions -R, denoting a Reduced average time (1 hour) for more Responsivity
testdata$PredNIS_4pR <- NA
testdata$PredNIS_8pR <- NA

#loop through each time
for (y in 2:length(testdata$Time)){
  #looking for 4pm
  if (as.POSIXlt(testdata$Time[y])$hour== 16 & as.POSIXlt(testdata$Time[y])$min == 0){
    testdata$PredNIS_4p[y] = mean(testdata$NIS[(y-11):y])
    testdata$PredNIS_4pR[y] = mean(testdata$NIS[(y-5):y])
  }
  #or times after 4pm in a given day
  else if (!(as.POSIXlt(testdata$Time[y])$hour %in% c(7,8,9,10,11,12,13,14,15))) {
    if(!is.na(testdata$PredNIS_4p[y-1])){
    testdata$PredNIS_4p[y] = testdata$PredNIS_4p[y-1] +
      delta$dNIS[as.POSIXlt(testdata$Time[y-1])$wday == as.POSIXlt(delta$Time)$wday &                                   as.POSIXlt(testdata$Time[y-1])$hour == as.POSIXlt(delta$Time)$hour &
                 as.POSIXlt(testdata$Time[y-1])$min == as.POSIXlt(delta$Time)$min]
    }
  
    if (!is.na(testdata$PredNIS_4pR[y-1])){
    testdata$PredNIS_4pR[y] = testdata$PredNIS_4pR[y-1] + 
      delta$dNIS[as.POSIXlt(testdata$Time[y-1])$wday == as.POSIXlt(delta$Time)$wday &                                   as.POSIXlt(testdata$Time[y-1])$hour == as.POSIXlt(delta$Time)$hour &
                 as.POSIXlt(testdata$Time[y-1])$min == as.POSIXlt(delta$Time)$min]
    }
  }

}
#do the same, except looking for and filling in 8pm predictions
for (y in 2:length(testdata$Time)){
  if (as.POSIXlt(testdata$Time[y])$hour== 20 & as.POSIXlt(testdata$Time[y])$min == 0){
    testdata$PredNIS_8p[y] = mean(testdata$NIS[(y-11):y])  
    testdata$PredNIS_8pR[y] = mean(testdata$NIS[(y-5):y])
  }
  else if (!(as.POSIXlt(testdata$Time[y])$hour %in% c(7,8,9,10,11,12,13,14,15,16,17,18,19))) {
    if (!is.na(testdata$PredNIS_8p[y-1])){
    testdata$PredNIS_8p[y] = testdata$PredNIS_8p[y-1] +
      delta$dNIS[as.POSIXlt(testdata$Time[y-1])$wday == as.POSIXlt(delta$Time)$wday &                                   as.POSIXlt(testdata$Time[y-1])$hour == as.POSIXlt(delta$Time)$hour &
                 as.POSIXlt(testdata$Time[y-1])$min == as.POSIXlt(delta$Time)$min]
    }
    if (!is.na(testdata$PredNIS_8pR[y-1])){
    testdata$PredNIS_8pR[y] = testdata$PredNIS_8pR[y-1] +
      delta$dNIS[as.POSIXlt(testdata$Time[y-1])$wday == as.POSIXlt(delta$Time)$wday &                                   as.POSIXlt(testdata$Time[y-1])$hour == as.POSIXlt(delta$Time)$hour &
                 as.POSIXlt(testdata$Time[y-1])$min == as.POSIXlt(delta$Time)$min]
    }
  }

}

#calculate occupancy predictions
testdata$PredOcc_4p <- testdata$PredNIS_4p/testdata$Rooms
testdata$PredOcc_8p <- testdata$PredNIS_8p/testdata$Rooms
testdata$PredOcc_4pR <- testdata$PredNIS_4pR/testdata$Rooms
testdata$PredOcc_8pR <- testdata$PredNIS_8pR/testdata$Rooms

```

We want to systematically display how our forecasts match up to actual NIS progressions in the test data. This entails adopting the delayed "day" convention discussed earlier. Again, in this scheme we define a day as the 24 hours separating consecutive 7am's. The following loop creates a list of slices of the test data, indexed by "day". One simple synthesis of the procedure is a plot of each day's true NIS progression versus a forecast for the same initiated at 4 and 8pm.

```{r, eval = TRUE, warning  = FALSE, echo  = FALSE, fig.height = 8, fig.width=8, fig.align='center', fig.cap = "True NIS versus forecasts; Blue Area =  True NIS, Orange Line = 4pm forecast, Pink Line = 8pm forecast."}
days <- list()
plots <- list()

for (v in 1:(length(unique(testdata$dday))-1)){
    days[[v]] <- testdata[testdata$dday == (v+28),]
    plots[[v]] <- ggplot(days[[v]], aes(Time, NIS)) + geom_area(fill = "blue")  + geom_step(aes(Time, PredNIS_4p), na.rm = TRUE, color = "orange") + geom_step(aes(Time, PredNIS_8p), na.rm=TRUE, color = "pink") + ylim(0,70) + theme(axis.title.x= element_blank(), axis.title.y = element_blank(), axis.text.x = element_blank(), axis.text.y = element_blank())
}


invisible(multiplot(plotlist = plots, layout = matrix(c(1:56), nrow = 8, byrow = TRUE)))
```

This visualization can help to add color or a "feel" to how well our forecasts fare using this particular arrangement of forecast. There are many parameters we can choose that may affect the forecast. Do we forecast at set times or several hours before a known maximum? 2 hours before? More or Less? Do we forecast several hours before every instance of our output (NIS in the preceding case) above some threshold? Do we need to lengthen or reduce the length-of-average for our forecast seed? Do we forecast for Occupancy Rate instead? How about Modified Occupancy Rate? 

With all these design choices, we need a simpler tool to evaluate alternative forecasts.I propose three measures to evalue quality of a forecast. For each day in the test data, we should compare the time of the maximum NIS with the time suggested by a prediction. Additionally, we should compare the magnitude of the forecasted maximum NIS versus the actual maximum. Finally, as a measure of the quality of the forecast over its entire span, we should calculate the area between the forecasted NIS curve and the actual NIS curve. While I did not have time to finish the coding, a sample of how the first might work is below. 
```{r eval=FALSE}
#make a list for each of our measures
timediff <- list()
magdiff <- list()
AUC <- list()

#cycle through each day in the measure
for (o in min(testdata$dday):(max(testdata$dday)-1)){
  #get the ED data associated with that day
  d <- days[[o]]
  #calculate the difference between time of actual max and time of forecasted max
  timediff[mean(d$dday)] <- difftime(d$Time[which(d$NIS == max(d$NIS, na.rm = TRUE))[1]], d$Time[which(d$PredNIS_4p == max(d$PredNIS_4p, na.rm = TRUE))[1]])
  #calculate difference between actual max NIS and forecasted max NIS (absolute value)
  
  #use nested for loop to calculate AUC (actually a Riemann sum since time is discrete)
  
  #calculate boolean of forecast (Y/N Surge) and actual (Y\N Surge)
  
  
}

#format peformance measures into a 8x7 Matrix

```

My plan is to orient (using absolute value) all measures so that they fall on the interval [0, $\infty$), where 0 is a perfect score. I will also format the performance results in 8x7 matrices, one for each (3x) performance measure. This layout mirrors the subplots of the previous visualization. So, by matching the subplot in position i,j of the visualization with each data value in position i, j of the three performance measure matrices, the reader will have a rich depiction of how well the forecast fared on that day. Furthermore, the reader can ascertain the performance of the particular forecasting method as a whole by applying the appropriate summary statistic to the performance measures (sum, average, or median), and looking at the full visualization for general trends.

Since there are a multitude of parameters to choose from in our forecasts, it is important to establish a systematic tool of comparing performance in this way. I encourage any ideas about how to improve what I have here, either in methodology of implementation. Following that discussion, I can begin to compare the forecasting alternatives.


##Forecast Parameter Dimensions

####Objective

1. Daily Maximum

2. First Surge

####Timeframe

i) Bihourly Huddle

ii) Recurring

iii) Static Time


####Output Metric
  
A. NIS

B. Occupancy

C. NIS-to-Occupancy

D. Modified Occupancy

####Seed

In order reduce the variance of the initial reading of our output measure at the outset of a forecast, we average it over an amount of preceding time. This seed can be averaged over several minutes or hours.

###Details
1. In this scenario, our forecast seeks to identify the time and magnitude of the maximum daily output measure (NIS, Occupancy etc.) This type of forecast is helpful for planning in general but may be less suited to surge prediction in particular

2. In this scenario, we seek to forecast the first instance of Surge of the day, regardless of if this corresponds to the daily max. A consistent definition of Surge is a prerequisite, although varying levels (Occ = 1.5, 1.6 etc.) can be explored. This objective will reduce the sample size of days in our test data, since a day will not necessarily meet our definition of Surge.

i) This scenario proposes that each ED Huddle incorporates a Surge prediction. We will represent this scenario for our alternative comparison as a prediction levied 2 hours before the objective above. In operation, the true time lag will be a random variable distributed on [0, 2).  **A preferred analytical representation would be possible with knowledge of Huddle times (are they every hour, on the hour?). With this knowledge we could make a forecast at the last huddle before the objective.

ii) This scenario is identical to the Bihourly Huddle Prediction in both operation and our analytical representation. The difference is the frequency of predictions, which may be every 1, 3, or 4 hours rather than every 2 hours.

iv) This scenario emphasizes critical "decision times", likely related to staffing level changes. 4pm and 8pm were previously identified as candidate decision times. 
  
A. Number in System is the simplest output measure, defined as the number of patients who have arrived but not yet left at any given time. 

B. Occupancy Rate is defined as the Number in System divided by the number of staffed ED Rooms at any given time. 

C. A combination of NIS and Occupancy can also be used. NIS will be used for the regression, but the final results will be calculated based on a conversion into Occupancy Rate. 

D. Modified Occupancy Rate takes the original denominator and turns it into a time-weighted average of the number of rooms available in the near future. 





